#!/bin/bash

# Databricks Asset Bundle Setup Script
set -e

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
BLUE='\033[0;34m'
NC='\033[0m'

print_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
print_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
print_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Check prerequisites and select profile
check_prerequisites() {
    print_info "Checking prerequisites..."
    
    if ! command -v databricks &> /dev/null; then
        print_error "Databricks CLI not found. Run 'databricks-workspace-setup.sh' first"
        exit 1
    fi
    
    if ! command -v python3.11 &> /dev/null; then
        print_error "Python 3.11 not found"
        exit 1
    fi
    
    # Check for profiles and let user select
    if [ -f ~/.databrickscfg ]; then
        print_info "Available workspace profiles:"
        profiles=$(grep "\[" ~/.databrickscfg | tr -d '[]' | grep -v DEFAULT)
        if [ -n "$profiles" ]; then
            # Group and display profiles by environment
            echo ""
            for env in dit fit iat; do
                env_profiles=$(echo "$profiles" | grep "^$env")
                if [ -n "$env_profiles" ]; then
                    echo "$env Environment:"
                    echo "$env_profiles" | nl -w2 -s') ' | sed 's/^/  /'
                    echo ""
                fi
            done
            
            echo "All profiles:"
            echo "$profiles" | nl -w2 -s') '
            echo ""
            read -p "Select profile number for this project [1]: " profile_num
            profile_num=${profile_num:-1}
            
            selected_profile=$(echo "$profiles" | sed -n "${profile_num}p")
            if [ -n "$selected_profile" ]; then
                export DATABRICKS_CONFIG_PROFILE="$selected_profile"
                print_success "Using profile: $selected_profile"
                
                # Extract environment from profile name
                env_name=$(echo "$selected_profile" | cut -d'-' -f1)
                workspace_desc=$(echo "$selected_profile" | cut -d'-' -f2-)
                print_info "Environment: $env_name, Workspace: $workspace_desc"
            else
                print_error "Invalid profile selection"
                exit 1
            fi
        else
            print_error "No profiles found. Run 'databricks-workspace-setup.sh' first"
            exit 1
        fi
    else
        print_error "No Databricks configuration found. Run 'databricks-workspace-setup.sh' first"
        exit 1
    fi
    
    print_success "Prerequisites met"
}

# Create project
create_project() {
    print_info "Creating Databricks bundle project..."
    
    read -p "Project name [my-databricks-project]: " project_name
    project_name=${project_name:-my-databricks-project}
    
    if [ -d "$project_name" ]; then
        print_error "Directory $project_name already exists"
        exit 1
    fi
    
    mkdir -p "$project_name"
    cd "$project_name"
    
    # Initialize bundle with profile
    if [ -n "$DATABRICKS_CONFIG_PROFILE" ]; then
        DATABRICKS_CONFIG_PROFILE="$DATABRICKS_CONFIG_PROFILE" databricks bundle init default-python --output-dir .
    else
        databricks bundle init default-python --output-dir .
    fi
    
    # Update databricks.yml with multiple environments
    update_bundle_config
    
    print_success "Project created"
}

# Update bundle configuration for multiple environments
update_bundle_config() {
    if [ -f "databricks.yml" ]; then
        # Add deployment targets matching profiles (dit/fit/iat)
        cat >> databricks.yml << EOF

# Deployment targets
targets:
  dit:
    mode: development
    # Uses dit profile
  
  fit:
    mode: development
    # Uses fit profile
  
  iat:
    mode: production
    # Uses iat profile
EOF
    fi
}

# Configure VS Code
configure_vscode() {
    print_info "Configuring VS Code..."
    
    mkdir -p .vscode
    
    cat > .vscode/settings.json << 'EOF'
{
    "python.defaultInterpreterPath": "${workspaceFolder}/.venv/bin/python",
    "python.pythonPath": "${workspaceFolder}/.venv/bin/python",
    "databricks.sync.destinationType": "workspace",
    "databricks.sync.remoteWatchEnabled": true,
    "python.testing.pytestEnabled": true,
    "python.testing.pytestArgs": ["tests/"]
}
EOF
    
    cat > .vscode/extensions.json << 'EOF'
{
    "recommendations": [
        "databricks.databricks",
        "ms-python.python",
        "ms-toolsai.jupyter"
    ]
}
EOF
    
    print_success "VS Code configured"
}

# Create virtual environment
create_virtualenv() {
    print_info "Creating Python virtual environment..."
    
    python3.11 -m venv .venv
    source .venv/bin/activate
    
    pip install --upgrade pip
    pip install databricks-sdk pytest
    
    cat > activate.sh << 'EOF'
#!/bin/bash
source .venv/bin/activate
echo "Virtual environment activated"
EOF
    chmod +x activate.sh
    
    print_success "Virtual environment created"
}

# Create example notebook
create_examples() {
    print_info "Creating example notebook..."
    
    mkdir -p notebooks
    cat > notebooks/example.py << 'EOF'
# Databricks notebook source
# MAGIC %md
# MAGIC # Example Notebook

# COMMAND ----------

from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Create sample data
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
df = spark.createDataFrame(data, ["name", "age"])
display(df)

# COMMAND ----------

# Basic transformations
result = df.filter(col("age") > 25).select("name")
display(result)
EOF
    
    print_success "Example notebook created"
}

# Create deployment scripts
create_scripts() {
    print_info "Creating deployment scripts..."
    
    cat > deploy.sh << 'EOF'
#!/bin/bash
TARGET=${1:-dit}
PROFILE=${2:-$TARGET}

if [ -n "$PROFILE" ]; then
    export DATABRICKS_CONFIG_PROFILE="$PROFILE"
fi

echo "Deploying to $TARGET using profile $PROFILE..."
databricks bundle validate -t $TARGET
databricks bundle deploy -t $TARGET
echo "Deployment complete"
EOF
    chmod +x deploy.sh
    
    cat > sync.sh << 'EOF'
#!/bin/bash
TARGET=${1:-dit}
PROFILE=${2:-$TARGET}

if [ -n "$PROFILE" ]; then
    export DATABRICKS_CONFIG_PROFILE="$PROFILE"
fi

echo "Starting sync to $TARGET using profile $PROFILE..."
databricks bundle sync -t $TARGET --watch
EOF
    chmod +x sync.sh
    
    # Create profile-specific scripts
    cat > deploy-with-profile.sh << 'EOF'
#!/bin/bash
echo "Available environments: dit, fit, iat"
read -p "Select environment: " TARGET
read -p "Use specific profile (leave empty to match target): " PROFILE

if [ -z "$PROFILE" ]; then
    PROFILE="$TARGET"
fi

./deploy.sh "$TARGET" "$PROFILE"
EOF
    chmod +x deploy-with-profile.sh
    
    print_success "Scripts created"
}

# Create basic tests
create_tests() {
    print_info "Creating test structure..."
    
    mkdir -p tests
    cat > tests/test_example.py << 'EOF'
import pytest
from pyspark.sql import SparkSession

@pytest.fixture(scope="session")
def spark():
    return SparkSession.builder.appName("test").master("local[*]").getOrCreate()

def test_basic_spark(spark):
    data = [("test", 1)]
    df = spark.createDataFrame(data, ["name", "value"])
    assert df.count() == 1
EOF
    
    print_success "Tests created"
}

# Main execution
main() {
    echo "========================================"
    echo "  Databricks Bundle Project Setup"
    echo "========================================"
    echo ""
    if [ -n "$DATABRICKS_CONFIG_PROFILE" ]; then
        echo "Using profile: $DATABRICKS_CONFIG_PROFILE"
    fi
    echo ""
    
    check_prerequisites
    create_project
    configure_vscode
    create_virtualenv
    create_examples
    create_scripts
    create_tests
    
    echo ""
    print_success "Project setup complete!"
    echo ""
    echo "Next steps:"
    echo "1. cd $(basename $(pwd))"
    echo "2. source activate.sh"
    echo "3. code . (open in VS Code)"
    echo "4. ./deploy.sh dev"
    echo "5. ./sync.sh dev"
}

main "$@"
